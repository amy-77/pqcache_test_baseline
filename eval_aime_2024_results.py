#!/usr/bin/env python3
"""
AIME 2024æ•°å­¦æ¨ç†ç»“æœè¯„ä¼°è„šæœ¬
å‚è€ƒR-KVç®—æ³•çš„è¯„ä¼°æ–¹æ³•ï¼Œå¯¹PQCacheç”Ÿæˆçš„AIME 2024æ¨ç†ç»“æœè¿›è¡Œå‡†ç¡®æ€§è¯„ä¼°
"""

import json
import re
from pathlib import Path
import argparse
from typing import Dict, List, Any, Tuple

def extract_answer_from_response(response: str, dataset_name: str = "aime_2024") -> str:
    """
    ä»æ¨¡å‹å“åº”ä¸­æå–æœ€ç»ˆç­”æ¡ˆ
    å‚è€ƒR-KVçš„ç­”æ¡ˆæå–é€»è¾‘ï¼Œä¼˜åŒ–æ•°å­¦ç­”æ¡ˆæå–
    """
    if not response:
        return ""
    
    # ä¼˜å…ˆçº§æ¨¡å¼ - boxedæ ¼å¼æœ€å¯é ï¼ˆAIMEæ ‡å‡†æ ¼å¼ï¼‰
    patterns = [
        r"\\boxed\{([^}]+)\}",  # LaTeX boxedæ ¼å¼ï¼ˆæœ€é«˜ä¼˜å…ˆçº§ï¼‰
        r"Final answer:\s*([+-]?\d+(?:\.\d+)?)",  # Final answeræ ¼å¼
        r"(?:final answer|answer|solution).*?(?:is|:|=)\s*([+-]?\d+(?:\.\d+)?)",
        r"(?:therefore|thus|so),?\s*(?:the answer is)?\s*([+-]?\d+(?:\.\d+)?)",
        r"\$([+-]?\d+(?:\.\d+)?)\$",  # LaTeXæ•°å­¦æ ¼å¼
        r"([+-]?\d+(?:\.\d+)?)(?:\s*$|\s*\.?\s*$)"  # æœ«å°¾æ•°å­—
    ]
    
    for pattern in patterns:
        if "boxed" in pattern:
            # å¯¹boxedæ ¼å¼ä¸è½¬æ¢ä¸ºå°å†™ï¼Œä¿æŒåŸå§‹å†…å®¹
            matches = re.findall(pattern, response)
        else:
            matches = re.findall(pattern, response.lower(), re.IGNORECASE)
        
        if matches:
            answer = matches[-1].strip()  # å–æœ€åä¸€ä¸ªåŒ¹é…
            
            # å¯¹boxedç­”æ¡ˆè¿›è¡Œç‰¹æ®Šå¤„ç†
            if "boxed" in pattern:
                # å°è¯•ä»å¤æ‚çš„boxedå†…å®¹ä¸­æå–æ•°å­—
                if answer.isdigit():
                    return answer
                else:
                    # å°è¯•æå–æ•°å­—
                    number_match = re.search(r'([+-]?\d+(?:\.\d+)?)', answer)
                    if number_match:
                        return number_match.group(1)
                    else:
                        return answer  # å¦‚æœæ— æ³•æå–æ•°å­—ï¼Œè¿”å›åŸå§‹å†…å®¹
            else:
                return answer
    
    return ""

def normalize_answer(answer: str) -> str:
    """æ ‡å‡†åŒ–ç­”æ¡ˆæ ¼å¼ï¼Œç”¨äºæ¯”è¾ƒ"""
    if not answer:
        return ""
    
    # ç§»é™¤å¤šä½™çš„ç©ºæ ¼å’Œæ ‡ç‚¹
    answer = answer.strip().rstrip('.')
    
    # å¯¹äºAIMEï¼Œç­”æ¡ˆé€šå¸¸æ˜¯æ•´æ•°
    try:
        # å°è¯•è½¬æ¢ä¸ºæ•´æ•°ï¼ˆAIMEç­”æ¡ˆèŒƒå›´0-999ï¼‰
        num = float(answer)
        if num == int(num) and 0 <= int(num) <= 999:
            return str(int(num))
    except ValueError:
        pass
    
    return answer

def evaluate_accuracy(predicted: str, ground_truth: str) -> bool:
    """
    è¯„ä¼°é¢„æµ‹ç­”æ¡ˆçš„å‡†ç¡®æ€§
    ä½¿ç”¨æ›´ä¸¥æ ¼çš„åŒ¹é…æ ‡å‡†ï¼Œå‚è€ƒR-KVçš„è¯„ä¼°æ–¹æ³•
    """
    pred_norm = normalize_answer(predicted)
    gt_norm = normalize_answer(ground_truth)
    
    if not pred_norm or not gt_norm:
        return False
    
    # ç²¾ç¡®åŒ¹é…
    return pred_norm == gt_norm

def calculate_detailed_metrics(results: List[Dict]) -> Dict[str, Any]:
    """è®¡ç®—è¯¦ç»†çš„è¯„ä¼°æŒ‡æ ‡"""
    total = len(results)
    if total == 0:
        return {"error": "No results to evaluate"}
    
    correct = sum(1 for r in results if r.get('is_correct', False))
    
    # æŒ‰é—®é¢˜IDåˆ†æï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
    by_problem_id = {}
    generation_times = []
    input_lengths = []
    output_lengths = []
    
    for result in results:
        problem_id = result.get('id', 'unknown')
        by_problem_id[problem_id] = result.get('is_correct', False)
        
        if 'generation_time' in result:
            generation_times.append(result['generation_time'])
        if 'input_length' in result:
            input_lengths.append(result['input_length'])
        if 'output_length' in result:
            output_lengths.append(result['output_length'])
    
    metrics = {
        "total_samples": total,
        "correct_samples": correct,
        "accuracy": correct / total,
        "error_rate": 1 - (correct / total),
        "by_problem_accuracy": by_problem_id
    }
    
    # æ€§èƒ½æŒ‡æ ‡
    if generation_times:
        metrics["avg_generation_time"] = sum(generation_times) / len(generation_times)
        metrics["total_generation_time"] = sum(generation_times)
    
    if input_lengths:
        metrics["avg_input_length"] = sum(input_lengths) / len(input_lengths)
    
    if output_lengths:
        metrics["avg_output_length"] = sum(output_lengths) / len(output_lengths)
    
    return metrics

def re_evaluate_results(results_file: str, output_file: str = None) -> Tuple[Dict, List[Dict]]:
    """
    é‡æ–°è¯„ä¼°å·²æœ‰çš„æ¨ç†ç»“æœï¼Œä½¿ç”¨æ”¹è¿›çš„ç­”æ¡ˆæå–å’ŒåŒ¹é…é€»è¾‘
    """
    print(f"ğŸ“Š é‡æ–°è¯„ä¼°ç»“æœæ–‡ä»¶: {results_file}")
    
    # è¯»å–ç»“æœæ–‡ä»¶
    results = []
    with open(results_file, 'r', encoding='utf-8') as f:
        for line in f:
            line = line.strip()
            if line:
                results.append(json.loads(line))
    
    print(f"ğŸ“ˆ åŠ è½½äº† {len(results)} ä¸ªæ ·æœ¬")
    
    # é‡æ–°è¯„ä¼°æ¯ä¸ªæ ·æœ¬
    re_evaluated_results = []
    correct_count = 0
    
    for i, result in enumerate(results):
        # è·å–åŸå§‹æ•°æ®
        problem = result.get('problem', '')
        ground_truth = result.get('ground_truth', '')
        full_response = result.get('full_response', '')
        
        # é‡æ–°æå–é¢„æµ‹ç­”æ¡ˆ
        predicted_answer = extract_answer_from_response(full_response, "aime_2024")
        
        # é‡æ–°è¯„ä¼°å‡†ç¡®æ€§
        is_correct = evaluate_accuracy(predicted_answer, ground_truth)
        
        if is_correct:
            correct_count += 1
        
        # æ›´æ–°ç»“æœ
        updated_result = result.copy()
        updated_result['predicted_answer_re_extracted'] = predicted_answer
        updated_result['is_correct_re_evaluated'] = is_correct
        updated_result['original_predicted'] = result.get('predicted_answer', '')
        updated_result['original_is_correct'] = result.get('is_correct', False)
        
        re_evaluated_results.append(updated_result)
        
        # æ˜¾ç¤ºå‰å‡ ä¸ªæ ·æœ¬çš„è¯¦ç»†ä¿¡æ¯
        if i < 5:
            print(f"\næ ·æœ¬ {i+1}:")
            print(f"  é—®é¢˜: {problem[:100]}...")
            print(f"  æ ‡å‡†ç­”æ¡ˆ: {ground_truth}")
            print(f"  åŸé¢„æµ‹ç­”æ¡ˆ: {result.get('predicted_answer', '')}")
            print(f"  é‡æ–°æå–ç­”æ¡ˆ: {predicted_answer}")
            print(f"  åŸæ­£ç¡®æ€§: {result.get('is_correct', False)}")
            print(f"  é‡æ–°è¯„ä¼°æ­£ç¡®æ€§: {is_correct}")
    
    # è®¡ç®—è¯¦ç»†æŒ‡æ ‡
    metrics = calculate_detailed_metrics(re_evaluated_results)
    
    # æ¯”è¾ƒåŸå§‹è¯„ä¼°å’Œé‡æ–°è¯„ä¼°çš„ç»“æœ
    original_correct = sum(1 for r in results if r.get('is_correct', False))
    
    print(f"\n{'='*60}")
    print(f"ğŸ” AIME 2024 é‡æ–°è¯„ä¼°ç»“æœå¯¹æ¯”")
    print(f"{'='*60}")
    print(f"æ€»æ ·æœ¬æ•°: {len(results)}")
    print(f"åŸå§‹è¯„ä¼° - æ­£ç¡®: {original_correct}, å‡†ç¡®ç‡: {original_correct/len(results):.4f}")
    print(f"é‡æ–°è¯„ä¼° - æ­£ç¡®: {correct_count}, å‡†ç¡®ç‡: {correct_count/len(results):.4f}")
    print(f"å‡†ç¡®ç‡å˜åŒ–: {(correct_count - original_correct)/len(results):+.4f}")
    
    if metrics.get('avg_generation_time'):
        print(f"å¹³å‡ç”Ÿæˆæ—¶é—´: {metrics['avg_generation_time']:.2f}ç§’")
    if metrics.get('avg_input_length'):
        print(f"å¹³å‡è¾“å…¥é•¿åº¦: {metrics['avg_input_length']:.0f} tokens")
    if metrics.get('avg_output_length'):
        print(f"å¹³å‡è¾“å‡ºé•¿åº¦: {metrics['avg_output_length']:.0f} tokens")
    
    # ä¿å­˜é‡æ–°è¯„ä¼°çš„ç»“æœ
    if output_file:
        with open(output_file, 'w', encoding='utf-8') as f:
            for result in re_evaluated_results:
                f.write(json.dumps(result, ensure_ascii=False) + '\n')
        print(f"âœ… é‡æ–°è¯„ä¼°ç»“æœå·²ä¿å­˜åˆ°: {output_file}")
    
    return metrics, re_evaluated_results

def analyze_error_patterns(results: List[Dict]) -> Dict[str, Any]:
    """åˆ†æé”™è¯¯æ¨¡å¼ï¼Œå¸®åŠ©æ”¹è¿›æ¨¡å‹"""
    
    errors = [r for r in results if not r.get('is_correct_re_evaluated', False)]
    
    if not errors:
        return {"message": "æ²¡æœ‰é”™è¯¯æ ·æœ¬è¿›è¡Œåˆ†æ"}
    
    error_analysis = {
        "total_errors": len(errors),
        "error_types": {},
        "common_patterns": [],
        "answer_extraction_failures": 0,
        "wrong_calculations": 0
    }
    
    for error in errors:
        predicted = error.get('predicted_answer_re_extracted', '')
        ground_truth = error.get('ground_truth', '')
        response = error.get('full_response', '')
        
        # åˆ†æé”™è¯¯ç±»å‹
        if not predicted:
            error_analysis["answer_extraction_failures"] += 1
        elif predicted != ground_truth:
            error_analysis["wrong_calculations"] += 1
        
        # åˆ†æå“åº”é•¿åº¦ï¼ˆå¯èƒ½æŒ‡ç¤ºæ¨ç†æ·±åº¦ï¼‰
        response_length = len(response)
        if response_length < 100:
            error_analysis["error_types"]["too_short_response"] = error_analysis["error_types"].get("too_short_response", 0) + 1
        elif response_length > 5000:
            error_analysis["error_types"]["too_long_response"] = error_analysis["error_types"].get("too_long_response", 0) + 1
    
    return error_analysis

def main():
    parser = argparse.ArgumentParser(description="AIME 2024ç»“æœè¯„ä¼°è„šæœ¬")
    parser.add_argument('--results_file', type=str, required=True,
                        help="æ¨ç†ç»“æœæ–‡ä»¶è·¯å¾„")
    parser.add_argument('--output_file', type=str, default=None,
                        help="é‡æ–°è¯„ä¼°ç»“æœä¿å­˜è·¯å¾„")
    parser.add_argument('--detailed_analysis', action='store_true',
                        help="è¿›è¡Œè¯¦ç»†çš„é”™è¯¯åˆ†æ")
    
    args = parser.parse_args()
    
    # éªŒè¯æ–‡ä»¶å­˜åœ¨
    if not Path(args.results_file).exists():
        print(f"âŒ ç»“æœæ–‡ä»¶ä¸å­˜åœ¨: {args.results_file}")
        return
    
    # å¦‚æœæ²¡æœ‰æŒ‡å®šè¾“å‡ºæ–‡ä»¶ï¼Œç”Ÿæˆé»˜è®¤åç§°
    if not args.output_file:
        results_path = Path(args.results_file)
        args.output_file = str(results_path.parent / f"re_evaluated_{results_path.name}")
    
    # æ‰§è¡Œé‡æ–°è¯„ä¼°
    metrics, re_evaluated_results = re_evaluate_results(args.results_file, args.output_file)
    
    # è¯¦ç»†åˆ†æï¼ˆå¦‚æœè¦æ±‚ï¼‰
    if args.detailed_analysis:
        print(f"\nğŸ” è¿›è¡Œè¯¦ç»†é”™è¯¯åˆ†æ...")
        error_analysis = analyze_error_patterns(re_evaluated_results)
        
        print(f"\né”™è¯¯åˆ†æç»“æœ:")
        print(f"  æ€»é”™è¯¯æ•°: {error_analysis['total_errors']}")
        print(f"  ç­”æ¡ˆæå–å¤±è´¥: {error_analysis['answer_extraction_failures']}")
        print(f"  è®¡ç®—é”™è¯¯: {error_analysis['wrong_calculations']}")
        
        if error_analysis.get('error_types'):
            print(f"  é”™è¯¯ç±»å‹åˆ†å¸ƒ: {error_analysis['error_types']}")
    
    print(f"\nâœ… è¯„ä¼°å®Œæˆï¼")

if __name__ == "__main__":
    main()
