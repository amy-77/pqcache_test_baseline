#!/bin/bash
# Full-Attention MATH-500 æ•°å­¦æ¨ç†è¯„ä¼°
# å¯¹æ¯”AIME 2024çš„éš¾åº¦ï¼ŒéªŒè¯æ¨¡å‹åœ¨ç›¸å¯¹ç®€å•æ•°å­¦é¢˜ä¸Šçš„è¡¨ç°

set -e

# ç¡®ä¿æ¿€æ´»æ­£ç¡®çš„condaç¯å¢ƒ
source /home/pai/data/miniconda3/etc/profile.d/conda.sh
conda activate pqcache

echo "=========================================="
echo "Full-Attention Mathematics CoT - MATH-500"
echo "æ•°æ®é›†: MATH-500 (é«˜ä¸­æ ‡å‡†æ•°å­¦)"
echo "æ¨¡å‹: Llama-3.1-8B-Instruct"
echo "ä»»åŠ¡: Full-Attention Chain-of-Thought Mathematical Reasoning"
echo "=========================================="

# ä»»åŠ¡é…ç½®
DATASET="math_500"
MODEL="llama-3.1"
OUTPUT_DIR="math_cot_results"
MAX_SEQ_LENGTH=32768
MAX_NEW_TOKENS=2048
NUM_EVAL_EXAMPLES=20  # å…ˆæµ‹è¯•20ä¸ªæ ·æœ¬ï¼ŒéªŒè¯æ•ˆæœ
TEMPERATURE=0.0  # ç¡®å®šæ€§ç”Ÿæˆ

# Method configuration - ä½¿ç”¨full-attention
METHOD="fullattention"
COMPRESSOR="original"  # originalè¡¨ç¤ºä¸å‹ç¼©
DEVICE_ID=3

# è®¾ç½®ç¯å¢ƒå˜é‡
export CUDA_VISIBLE_DEVICES=${DEVICE_ID}
export HF_HOME=~/.cache/huggingface
export HF_ENDPOINT=https://hf-mirror.com

echo "æ¨¡å‹: ${MODEL}"
echo "æ•°æ®é›†: ${DATASET}"
echo "è®¾å¤‡: CUDA:${DEVICE_ID}"
echo "æ–¹æ³•: ${METHOD} (compressor: ${COMPRESSOR})"
echo "æœ€å¤§æ–°tokenæ•°: ${MAX_NEW_TOKENS}"
echo "è¯„ä¼°æ ·æœ¬æ•°: ${NUM_EVAL_EXAMPLES}"
echo "é¢˜ç›®éš¾åº¦: é«˜ä¸­æ ‡å‡†æ•°å­¦ (æ¯”AIMEç«èµ›ç®€å•)"
echo ""

# è¿è¡Œè¯„ä¼°
PYTORCH_CUDA_ALLOC_CONF="max_split_size_mb:128" \
TOKENIZERS_PARALLELISM=false \
python run_math500_cot_fullattention.py \
    --model ${MODEL} \
    --dataset ${DATASET} \
    --method ${METHOD} \
    --compressor ${COMPRESSOR} \
    --max_seq_length ${MAX_SEQ_LENGTH} \
    --max_new_tokens ${MAX_NEW_TOKENS} \
    --temperature ${TEMPERATURE} \
    --top_p 1.0 \
    --num_eval_examples ${NUM_EVAL_EXAMPLES} \
    --device_id ${DEVICE_ID} \
    --output_dir ${OUTPUT_DIR} \
    --verbose

echo ""
echo "âœ… MATH-500 Full-Attentionè¯„ä¼°å®Œæˆï¼"

# æ˜¾ç¤ºç»“æœæ–‡ä»¶ä½ç½®
RESULT_FILE="${OUTPUT_DIR}/${MODEL}/${DATASET}/fullattention_cot/cot_results_fullattention.jsonl"
if [ -f "$RESULT_FILE" ]; then
    echo "ğŸ“Š ç»“æœæ–‡ä»¶: $RESULT_FILE"
    echo "ğŸ“ˆ æ ·æœ¬æ•°é‡: $(wc -l < "$RESULT_FILE")"
    
    # å¿«é€Ÿç»Ÿè®¡æ­£ç¡®ç‡
    python -c "
import json
correct = 0
total = 0
with open('$RESULT_FILE', 'r') as f:
    for line in f:
        result = json.loads(line.strip())
        total += 1
        if result.get('is_correct', False):
            correct += 1
print(f'ğŸ¯ MATH-500å¿«é€Ÿç»Ÿè®¡ - æ­£ç¡®: {correct}/{total}, å‡†ç¡®ç‡: {correct/total:.4f} ({correct/total*100:.2f}%)')
print(f'ğŸ“Š å¯¹æ¯”AIME 2024: ä¹‹å‰AIMEå‡†ç¡®ç‡ä»…ä¸º3.33% (1/30)')
"
else
    echo "âŒ ç»“æœæ–‡ä»¶æœªæ‰¾åˆ°: $RESULT_FILE"
fi

echo ""
echo "ğŸ” è¦è¿›è¡Œè¯¦ç»†åˆ†æï¼Œè¯·è¿è¡Œ:"
echo "python eval_aime_2024_results.py --results_file $RESULT_FILE --detailed_analysis"

echo ""
echo "ğŸ“ å®éªŒç›®çš„éªŒè¯ï¼š"
echo "1. MATH-500ç›¸å¯¹AIME 2024æ˜¯å¦æ›´å®¹æ˜“ï¼Ÿ"
echo "2. Llama-3.1-8Båœ¨æ ‡å‡†é«˜ä¸­æ•°å­¦ä¸Šçš„åŸºçº¿è¡¨ç°å¦‚ä½•ï¼Ÿ"
echo "3. éªŒè¯PQCache vs Full-Attentionåœ¨åˆé€‚éš¾åº¦æ•°æ®é›†ä¸Šçš„å¯¹æ¯”æ•ˆæœ"
