#!/usr/bin/env python3
"""
Download LongBench dataset for PQCache project
"""
import os
import json
from datasets import load_dataset

def download_longbench():
    """Download LongBench dataset to ./data/ directory"""
    print("å¼€å§‹ä¸‹è½½ LongBench æ•°æ®é›†...")
    
    # åˆ›å»ºdataç›®å½•ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
    os.makedirs("./data", exist_ok=True)
    
    # è®¾ç½®ç¯å¢ƒå˜é‡ä½¿ç”¨é•œåƒæº
    os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'
    
    # æ ¹æ®å®˜æ–¹æ–‡æ¡£å®šä¹‰çš„æ•°æ®é›†åˆ—è¡¨
    datasets = ["narrativeqa", "qasper", "multifieldqa_en", "multifieldqa_zh", "hotpotqa", "2wikimqa", "musique",
                "dureader", "gov_report", "qmsum", "multi_news", "vcsum", "trec", "triviaqa", "samsum", "lsht",
                "passage_count", "passage_retrieval_en", "passage_retrieval_zh", "lcc", "repobench-p"]
    
    try:
        print("æ­£åœ¨ä» zai-org/LongBench ä¸‹è½½æ•°æ®é›†...")
        
        for dataset_name in datasets:
            try:
                print(f"æ­£åœ¨ä¸‹è½½ {dataset_name} æ•°æ®é›†...")
                # ä½¿ç”¨zai-orgé•œåƒæº
                data = load_dataset('zai-org/LongBench', dataset_name, split='test', trust_remote_code=True)
                
                output_file = f"./data/{dataset_name}.jsonl"
                print(f"æ­£åœ¨ä¿å­˜ {dataset_name} åˆ° {output_file}ï¼Œå…± {len(data)} ä¸ªæ ·æœ¬")
                
                with open(output_file, 'w', encoding='utf-8') as f:
                    for item in data:
                        f.write(json.dumps(item, ensure_ascii=False) + '\n')
                        
                print(f"âœ… {dataset_name} ä¸‹è½½å®Œæˆï¼")
                
            except Exception as e:
                print(f"âŒ {dataset_name} ä¸‹è½½å¤±è´¥: {e}")
                continue
        
        print(f"\nğŸ‰ LongBench æ•°æ®é›†ä¸‹è½½å®Œæˆï¼")
        print(f"æ•°æ®é›†å·²ä¿å­˜åˆ° ./data/ ç›®å½•")
        
    except Exception as e:
        print(f"âŒ æ•´ä½“ä¸‹è½½å¤±è´¥ï¼š{e}")
        print("æ­£åœ¨å°è¯•åˆ›å»ºæµ‹è¯•æ•°æ®...")
        
        # å¦‚æœä¸‹è½½å¤±è´¥ï¼Œåˆ›å»ºåŸºæœ¬çš„æµ‹è¯•æ•°æ®
        test_datasets = ['narrativeqa', 'qasper', 'multifieldqa_en', 'hotpotqa', '2wikimqa']
        for dataset_name in test_datasets:
            output_file = f"./data/{dataset_name}.jsonl"
            print(f"åˆ›å»ºæµ‹è¯•æ•°æ®æ–‡ä»¶: {output_file}")
            
            with open(output_file, 'w', encoding='utf-8') as f:
                # åˆ›å»ºä¸€ä¸ªç®€å•çš„æµ‹è¯•æ ·æœ¬
                test_sample = {
                    "input": f"What is the main topic of {dataset_name}?",
                    "context": f"This is a test context for {dataset_name} dataset. " * 50,
                    "answers": [f"{dataset_name} test answer"],
                    "length": 1000,
                    "dataset": dataset_name,
                    "language": "en",
                    "_id": f"test_{dataset_name}_001"
                }
                f.write(json.dumps(test_sample, ensure_ascii=False) + '\n')
        
        print("âœ… æµ‹è¯•æ•°æ®åˆ›å»ºå®Œæˆï¼")

if __name__ == "__main__":
    download_longbench() 