#!/usr/bin/env python3
"""
SCBenchè¯„ä¼°æŒ‡æ ‡è®¡ç®—è„šæœ¬
è®¡ç®—PQCacheåœ¨SCBenchæ•°æ®é›†ä¸Šçš„æ€§èƒ½æŒ‡æ ‡
"""
import os
import json
import argparse
import numpy as np
from collections import defaultdict
import re
from typing import Dict, List, Any

def parse_args():
    parser = argparse.ArgumentParser()
    parser.add_argument('--results_dir', type=str, default='results_scbench', help='ç»“æœç›®å½•')
    parser.add_argument('--dataset', type=str, default=None, help='æŒ‡å®šæ•°æ®é›†ï¼ŒNoneè¡¨ç¤ºå…¨éƒ¨')
    parser.add_argument('--output_file', type=str, default='scbench_evaluation.json', help='è¾“å‡ºæ–‡ä»¶')
    return parser.parse_args()

def extract_answer(text: str) -> str:
    """ä»ç”Ÿæˆæ–‡æœ¬ä¸­æå–ç­”æ¡ˆ"""
    # ç§»é™¤å¤šä½™çš„ç©ºç™½å­—ç¬¦
    text = text.strip()
    
    # å¦‚æœæ–‡æœ¬å¾ˆçŸ­ï¼Œç›´æ¥è¿”å›
    if len(text) < 10:
        return text
    
    # å°è¯•æå–å…³é”®ä¿¡æ¯ï¼ˆé’ˆå¯¹ä¸åŒä»»åŠ¡ç±»å‹ï¼‰
    # 1. ä»£ç ç›¸å…³ä»»åŠ¡ - æå–å‡½æ•°åæˆ–ä»£ç ç‰‡æ®µ
    code_patterns = [
        r'def\s+(\w+)',  # Pythonå‡½æ•°
        r'function\s+(\w+)',  # JavaScriptå‡½æ•°
        r'(\w+)\s*\(',  # å‡½æ•°è°ƒç”¨
    ]
    
    for pattern in code_patterns:
        matches = re.findall(pattern, text)
        if matches:
            return matches[0]
    
    # 2. é”®å€¼æŸ¥æ‰¾ä»»åŠ¡ - æå–å€¼
    kv_patterns = [
        r'"([^"]+)"',  # å¼•å·å†…çš„å€¼
        r':\s*([^\s,\]]+)',  # å†’å·åçš„å€¼
    ]
    
    for pattern in kv_patterns:
        matches = re.findall(pattern, text)
        if matches:
            return matches[0]
    
    # 3. é€šç”¨ç­”æ¡ˆæå– - å–ç¬¬ä¸€å¥è¯
    sentences = text.split('.')
    if sentences:
        return sentences[0].strip()
    
    return text[:100]  # æˆªæ–­åˆ°100å­—ç¬¦

def calculate_exact_match(pred: str, ref: str) -> float:
    """è®¡ç®—ç²¾ç¡®åŒ¹é…åˆ†æ•°"""
    pred_clean = pred.strip().lower()
    ref_clean = ref.strip().lower()
    return 1.0 if pred_clean == ref_clean else 0.0

def calculate_contains_match(pred: str, ref: str) -> float:
    """è®¡ç®—åŒ…å«åŒ¹é…åˆ†æ•°"""
    pred_clean = pred.strip().lower()
    ref_clean = ref.strip().lower()
    
    if ref_clean in pred_clean or pred_clean in ref_clean:
        return 1.0
    return 0.0

def calculate_token_overlap(pred: str, ref: str) -> float:
    """è®¡ç®—tokené‡å åˆ†æ•°"""
    pred_tokens = set(pred.lower().split())
    ref_tokens = set(ref.lower().split())
    
    if not ref_tokens:
        return 0.0
    
    overlap = len(pred_tokens & ref_tokens)
    return overlap / len(ref_tokens)

def evaluate_dataset_results(results_file: str, dataset_name: str) -> Dict[str, Any]:
    """è¯„ä¼°å•ä¸ªæ•°æ®é›†çš„ç»“æœ"""
    if not os.path.exists(results_file):
        return {'error': f'Results file not found: {results_file}'}
    
    # åŠ è½½ç»“æœ
    results = []
    with open(results_file, 'r', encoding='utf-8') as f:
        for line in f:
            if line.strip():
                results.append(json.loads(line.strip()))
    
    if not results:
        return {'error': 'No results found'}
    
    # è®¡ç®—å„ç§æŒ‡æ ‡
    metrics = {
        'exact_match': [],
        'contains_match': [],
        'token_overlap': [],
        'response_length': [],
        'input_length': [],
        'error_rate': 0,
        'total_samples': len(results)
    }
    
    error_count = 0
    
    for result in results:
        pred = result.get('response', '')
        ref = result.get('reference', '')
        
        # æ£€æŸ¥æ˜¯å¦æœ‰é”™è¯¯
        if pred.startswith('ERROR'):
            error_count += 1
            continue
        
        # æå–ç­”æ¡ˆ
        pred_answer = extract_answer(pred)
        ref_answer = extract_answer(ref)
        
        # è®¡ç®—æŒ‡æ ‡
        metrics['exact_match'].append(calculate_exact_match(pred_answer, ref_answer))
        metrics['contains_match'].append(calculate_contains_match(pred_answer, ref_answer))
        metrics['token_overlap'].append(calculate_token_overlap(pred_answer, ref_answer))
        
        # é•¿åº¦ç»Ÿè®¡
        metrics['response_length'].append(len(pred))
        metrics['input_length'].append(result.get('input_length', 0))
    
    # è®¡ç®—å¹³å‡å€¼
    metrics['error_rate'] = error_count / len(results)
    
    for key in ['exact_match', 'contains_match', 'token_overlap', 'response_length', 'input_length']:
        if metrics[key]:
            metrics[f'{key}_mean'] = np.mean(metrics[key])
            metrics[f'{key}_std'] = np.std(metrics[key])
        else:
            metrics[f'{key}_mean'] = 0.0
            metrics[f'{key}_std'] = 0.0
    
    # æŒ‰è½®æ¬¡åˆ†æï¼ˆå¦‚æœæ˜¯å¤šè½®å¯¹è¯ï¼‰
    turn_metrics = defaultdict(list)
    for result in results:
        turn = result.get('turn', 0)
        pred = result.get('response', '')
        ref = result.get('reference', '')
        
        if not pred.startswith('ERROR'):
            pred_answer = extract_answer(pred)
            ref_answer = extract_answer(ref)
            turn_metrics[turn].append({
                'exact_match': calculate_exact_match(pred_answer, ref_answer),
                'contains_match': calculate_contains_match(pred_answer, ref_answer),
                'token_overlap': calculate_token_overlap(pred_answer, ref_answer)
            })
    
    # è®¡ç®—æ¯è½®æ¬¡çš„å¹³å‡æŒ‡æ ‡
    turn_analysis = {}
    for turn, turn_results in turn_metrics.items():
        turn_analysis[f'turn_{turn}'] = {
            'exact_match': np.mean([r['exact_match'] for r in turn_results]),
            'contains_match': np.mean([r['contains_match'] for r in turn_results]),
            'token_overlap': np.mean([r['token_overlap'] for r in turn_results]),
            'sample_count': len(turn_results)
        }
    
    return {
        'dataset': dataset_name,
        'overall_metrics': metrics,
        'turn_analysis': turn_analysis
    }

def main():
    args = parse_args()
    
    print(f"ğŸ” å¼€å§‹è¯„ä¼°SCBenchç»“æœ")
    print(f"ğŸ“‚ ç»“æœç›®å½•: {args.results_dir}")
    
    if not os.path.exists(args.results_dir):
        print(f"âŒ ç»“æœç›®å½•ä¸å­˜åœ¨: {args.results_dir}")
        return
    
    all_evaluations = {}
    
    # éå†ç»“æœç›®å½•
    for item in os.listdir(args.results_dir):
        item_path = os.path.join(args.results_dir, item)
        
        if os.path.isdir(item_path):
            # æå–æ•°æ®é›†åç§°
            dataset_name = item.split('_')[0] + '_' + item.split('_')[1] + '_' + item.split('_')[2]
            
            # å¦‚æœæŒ‡å®šäº†ç‰¹å®šæ•°æ®é›†ï¼Œè·³è¿‡å…¶ä»–æ•°æ®é›†
            if args.dataset and args.dataset not in dataset_name:
                continue
            
            # æŸ¥æ‰¾ç»“æœæ–‡ä»¶
            results_file = os.path.join(item_path, 'results.jsonl')
            
            if os.path.exists(results_file):
                print(f"ğŸ“Š è¯„ä¼°æ•°æ®é›†: {dataset_name}")
                evaluation = evaluate_dataset_results(results_file, dataset_name)
                all_evaluations[dataset_name] = evaluation
                
                # æ‰“å°å…³é”®æŒ‡æ ‡
                if 'overall_metrics' in evaluation:
                    metrics = evaluation['overall_metrics']
                    print(f"  âœ… ç²¾ç¡®åŒ¹é…: {metrics.get('exact_match_mean', 0):.3f}")
                    print(f"  âœ… åŒ…å«åŒ¹é…: {metrics.get('contains_match_mean', 0):.3f}")
                    print(f"  âœ… Tokené‡å : {metrics.get('token_overlap_mean', 0):.3f}")
                    print(f"  âŒ é”™è¯¯ç‡: {metrics.get('error_rate', 0):.3f}")
                    print(f"  ğŸ“ å¹³å‡è¾“å…¥é•¿åº¦: {metrics.get('input_length_mean', 0):.0f}")
                    print(f"  ğŸ“ å¹³å‡è¾“å‡ºé•¿åº¦: {metrics.get('response_length_mean', 0):.0f}")
                else:
                    print(f"  âŒ è¯„ä¼°å¤±è´¥: {evaluation.get('error', 'Unknown error')}")
    
    # è®¡ç®—æ€»ä½“ç»Ÿè®¡
    if all_evaluations:
        overall_stats = {
            'total_datasets': len(all_evaluations),
            'avg_exact_match': np.mean([
                eval_data['overall_metrics'].get('exact_match_mean', 0) 
                for eval_data in all_evaluations.values() 
                if 'overall_metrics' in eval_data
            ]),
            'avg_contains_match': np.mean([
                eval_data['overall_metrics'].get('contains_match_mean', 0)
                for eval_data in all_evaluations.values()
                if 'overall_metrics' in eval_data
            ]),
            'avg_token_overlap': np.mean([
                eval_data['overall_metrics'].get('token_overlap_mean', 0)
                for eval_data in all_evaluations.values()
                if 'overall_metrics' in eval_data
            ])
        }
        
        all_evaluations['overall_summary'] = overall_stats
    
    # ä¿å­˜è¯„ä¼°ç»“æœ
    output_path = os.path.join(args.results_dir, args.output_file)
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(all_evaluations, f, indent=2, ensure_ascii=False)
    
    print(f"\nâœ… è¯„ä¼°å®Œæˆ!")
    print(f"ğŸ“Š è¯¦ç»†ç»“æœä¿å­˜åˆ°: {output_path}")
    
    if 'overall_summary' in all_evaluations:
        summary = all_evaluations['overall_summary']
        print(f"\nğŸ“ˆ æ€»ä½“ç»Ÿè®¡:")
        print(f"  ğŸ¯ å¹³å‡ç²¾ç¡®åŒ¹é…: {summary['avg_exact_match']:.3f}")
        print(f"  ğŸ¯ å¹³å‡åŒ…å«åŒ¹é…: {summary['avg_contains_match']:.3f}")  
        print(f"  ğŸ¯ å¹³å‡Tokené‡å : {summary['avg_token_overlap']:.3f}")
        print(f"  ğŸ“Š è¯„ä¼°æ•°æ®é›†æ•°é‡: {summary['total_datasets']}")

if __name__ == "__main__":
    main()
