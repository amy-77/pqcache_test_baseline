#!/usr/bin/env python3
"""
PQCacheè°ƒè¯•ç¤ºä¾‹ - è¿½è¸ªå‡½æ•°è°ƒç”¨æµç¨‹
ä½¿ç”¨è¿™ä¸ªè„šæœ¬æ¥ç†è§£ä»£ç æ‰§è¡Œè·¯å¾„
"""

import os
import sys
import torch
import numpy as np

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append('/home/pai/data/PQCache')

def debug_attention_flow():
    """è°ƒè¯•attentionè®¡ç®—æµç¨‹"""
    print("ğŸ¯ å¼€å§‹è°ƒè¯•Attentionæµç¨‹...")
    
    # æ¨¡æ‹Ÿåˆ›å»ºä¸€äº›ç®€å•çš„tensorç”¨äºè°ƒè¯•
    batch_size = 1
    seq_len = 10
    num_heads = 8
    head_dim = 64
    
    # åˆ›å»ºæ¨¡æ‹Ÿçš„query, key, value
    query = torch.randn(batch_size, num_heads, seq_len, head_dim)
    key = torch.randn(batch_size, num_heads, seq_len, head_dim)
    value = torch.randn(batch_size, num_heads, seq_len, head_dim)
    
    print(f"ğŸ“Š Query shape: {query.shape}")
    print(f"ğŸ“Š Key shape: {key.shape}")
    print(f"ğŸ“Š Value shape: {value.shape}")
    
    # è®¡ç®—attention weights
    attention_weights = torch.matmul(query, key.transpose(-2, -1)) / np.sqrt(head_dim)
    attention_weights = torch.softmax(attention_weights, dim=-1)
    
    print(f"ğŸ“Š Attention weights shape: {attention_weights.shape}")
    
    # è®¡ç®—Neff (æœ‰æ•ˆæ³¨æ„åŠ›æ•°)
    neff_values = []
    for head_idx in range(num_heads):
        head_weights = attention_weights[0, head_idx, :, :]  # [seq_len, seq_len]
        # å¯¹æ¯ä¸ªqueryä½ç½®è®¡ç®—Neff
        pos_neff = 1.0 / torch.clamp((head_weights ** 2).sum(dim=-1), min=1e-12)
        head_neff = pos_neff.mean().item()  # ç®€å•å¹³å‡
        neff_values.append(head_neff)
        print(f"ğŸ” Head {head_idx}: Neff = {head_neff:.3f}")
    
    return neff_values

def debug_import_pqcache():
    """è°ƒè¯•PQCacheæ¨¡å—å¯¼å…¥"""
    print("ğŸ¯ å¼€å§‹è°ƒè¯•PQCacheå¯¼å…¥...")
    
    try:
        # å°è¯•å¯¼å…¥PQCacheç›¸å…³æ¨¡å—
        from vq_method.llama31_patch import VQLlama31ForCausalLM
        print("âœ… æˆåŠŸå¯¼å…¥ VQLlama31ForCausalLM")
        
        from vq_method.retrieval_based.pq_search import PqBasedSearchCompressor
        print("âœ… æˆåŠŸå¯¼å…¥ PqBasedSearchCompressor")
        
        # æŸ¥çœ‹ç±»çš„æ–¹æ³•
        methods = [method for method in dir(PqBasedSearchCompressor) if not method.startswith('_')]
        print(f"ğŸ“‹ PqBasedSearchCompressor æ–¹æ³•: {methods}")
        
        return True
        
    except ImportError as e:
        print(f"âŒ å¯¼å…¥å¤±è´¥: {e}")
        return False

def debug_config_loading():
    """è°ƒè¯•é…ç½®åŠ è½½è¿‡ç¨‹"""
    print("ğŸ¯ å¼€å§‹è°ƒè¯•é…ç½®åŠ è½½...")
    
    # æ¨¡æ‹Ÿé…ç½®å¯¹è±¡
    class DebugConfig:
        def __init__(self):
            self.compressor = "pq_search"  # å¯ä»¥æ”¹æˆ "original" æˆ– "sparq_f"
            self.compress_ratio = 0.5
            self.recent_ratio = 0.1
            self.n_subvec_per_head = 4
            self.n_subbits = 8
            self.gqa = True
            self.sink_size = 32
            self.num_hidden_layers = 32
            self.num_key_value_heads = 8
            self.hidden_size = 4096
            self.num_attention_heads = 32
            self.max_iter = 100
    
    config = DebugConfig()
    print(f"ğŸ“‹ ä½¿ç”¨å‹ç¼©å™¨: {config.compressor}")
    print(f"ğŸ“‹ å‹ç¼©æ¯”ç‡: {config.compress_ratio}")
    
    return config

def main():
    """ä¸»è°ƒè¯•å‡½æ•°"""
    print("=" * 60)
    print("ğŸš€ PQCache è°ƒè¯•ä¼šè¯å¼€å§‹")
    print("=" * 60)
    
    # æ­¥éª¤1: è°ƒè¯•åŸºç¡€attentionè®¡ç®—
    print("\nğŸ” æ­¥éª¤1: è°ƒè¯•åŸºç¡€Attentionè®¡ç®—")
    neff_values = debug_attention_flow()
    
    # æ­¥éª¤2: è°ƒè¯•æ¨¡å—å¯¼å…¥
    print("\nğŸ” æ­¥éª¤2: è°ƒè¯•æ¨¡å—å¯¼å…¥")
    import_success = debug_import_pqcache()
    
    # æ­¥éª¤3: è°ƒè¯•é…ç½®
    print("\nğŸ” æ­¥éª¤3: è°ƒè¯•é…ç½®åŠ è½½")
    config = debug_config_loading()
    
    print("\n" + "=" * 60)
    print("âœ… è°ƒè¯•ä¼šè¯å®Œæˆ")
    print("=" * 60)
    
    # åœ¨è¿™é‡Œè®¾ç½®æ–­ç‚¹å¯ä»¥æ£€æŸ¥æ‰€æœ‰å˜é‡
    breakpoint_here = True  # åœ¨æ­¤è¡Œè®¾ç½®æ–­ç‚¹
    
if __name__ == "__main__":
    main() 